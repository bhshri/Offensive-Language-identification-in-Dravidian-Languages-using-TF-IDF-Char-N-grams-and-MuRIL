{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EACL Task Offencive Final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YNMrpRoDmRL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "!pip install pandarallel\n",
        "from pandarallel import pandarallel\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialization\n",
        "pandarallel.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gBnHP1D0Cu"
      },
      "source": [
        "# *Methods Function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCld4eNHD0_C"
      },
      "source": [
        "def data_analysis(data):\n",
        "  print(\"\\n\\ndata head\\n\\n\")\n",
        "  print(data.head())\n",
        "  label_count=data['label'].value_counts()\n",
        "  print(\"\\n\\nlabel count \\n\\n\")\n",
        "  print(label_count)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Number of sentence in data set : \",data[\"data\"].count())\n",
        "  print(\"Average words per sentence in data set:\", data['data'].str.split().str.len().mean())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBp0_S1pD3nv"
      },
      "source": [
        "def data_preprocess(data_set):\n",
        "  #print(data_set.head())\n",
        "   # #Pre processing train data\n",
        "  data_set['clean_text'] = data_set['data']\n",
        "  data_set['clean_text'] = data_set['clean_text'].str.replace(\"URL\",\"\")\n",
        "\n",
        "  data_set['clean_text'] = data_set['clean_text'].apply(lambda x: x.replace(\"@user\",\"\"))\n",
        "  # #remove retweet rt start of text\n",
        "  #data_set['clean_text'] = data_set['clean_text'].apply(lambda x: x.replace(\"rt \",\"\"))\n",
        "\n",
        "  # #remove single characcters\n",
        "  data_set['clean_text'] = data_set['clean_text'].apply(lambda x: re.sub(r'(?:^| )\\w(?:$| )', ' ', x).strip())\n",
        "\n",
        "  #Convert lower case\n",
        "  data_set['clean_text'] = data_set['clean_text'].apply(lambda x:x.lower())\n",
        "\n",
        "  # #remove emojis\n",
        "  data_set['clean_text'] = data_set['clean_text'].apply(lambda x:x.encode('ascii','ignore').decode('ascii'))\n",
        "  #remove digit\n",
        "  data_set['clean_text'] = data_set['clean_text'].apply(lambda x:re.sub(\" \\d+\", \" \", x))\n",
        "  # # #remove punctuations\n",
        "  punctuation='!!\"$%&()*+-/:;<=>?[\\\\]^_{|}~.#'\n",
        "  data_set['clean_text'] = data_set['clean_text'].parallel_apply(lambda x:''.join(ch for ch in x if ch not in set(punctuation)))\n",
        "  \n",
        "  return data_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcz0n4BGD7nS"
      },
      "source": [
        "def TFIDF_char_logistic(train,dev,test):\n",
        "\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(1, 3),analyzer='char')\n",
        "  full_text = list(train['clean_text'].values) + list(dev['clean_text'].values)\n",
        "\n",
        "  #tfidf = vectorizer.fit(full_text)\n",
        "\n",
        "  x_train_dev = vectorizer.fit_transform(full_text)\n",
        "  x_test = vectorizer.transform(test['clean_text'])\n",
        "\n",
        "  #x_train = tfidf.transform(train['data'])\n",
        "  y_train_dev= list(train['label'].values) + list(dev['label'].values)\n",
        "  #x_dev = tfidf.transform(dev['data'])\n",
        "  #y_dev= dev['label']\n",
        "\n",
        "  lr = LogisticRegression(max_iter=5000)\n",
        "  ovr = OneVsRestClassifier(lr)\n",
        "  ovr.fit(x_train_dev,y_train_dev)\n",
        "\n",
        "  print(\"Linear Regression Result char on Train\")\n",
        "  print(classification_report( y_train_dev,ovr.predict(x_train_dev) ))\n",
        "\n",
        "  # print(\"Linear Regression Result char on Test\")\n",
        "  # print(classification_report( ovr.predict(x_dev) , y_dev))\n",
        "  return ovr.predict(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO9iYDKHD--O"
      },
      "source": [
        "def TFIDF_char_svm(train,dev,test):\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(1, 3),analyzer='char')\n",
        "  full_text = list(train['clean_text'].values) + list(dev['clean_text'].values)\n",
        "\n",
        "  #tfidf = vectorizer.fit(full_text)\n",
        "\n",
        "  x_train_dev = vectorizer.fit_transform(full_text)\n",
        "  x_test = vectorizer.transform(test['clean_text'])\n",
        "\n",
        "  #x_train = tfidf.transform(train['data'])\n",
        "  y_train_dev= list(train['label'].values) + list(dev['label'].values)\n",
        "  #x_dev = tfidf.transform(dev['data'])\n",
        "  #y_dev= dev['label']\n",
        "\n",
        "  svm = SVC(kernel='linear',max_iter=5000) \n",
        "  svm.fit(x_train_dev,y_train_dev)\n",
        "\n",
        "  print(\"svm Result char on Train\")\n",
        "  print(classification_report( y_train_dev,svm.predict(x_train_dev) ))\n",
        "\n",
        "\n",
        "  #print(\"svm Result char on Test\")\n",
        "  #print(classification_report(svm.predict(x_dev),y_dev))\n",
        "  return svm.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd3a57IREBEQ"
      },
      "source": [
        "def TFIDF_char_randamforest_test(train,dev,test):\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(1, 3),analyzer='char')\n",
        "  full_text = list(train['clean_text'].values) + list(dev['clean_text'].values)\n",
        "\n",
        "  #tfidf = vectorizer.fit(full_text)\n",
        "\n",
        "  x_train_dev = vectorizer.fit_transform(full_text)\n",
        "  x_test = vectorizer.transform(test['clean_text'])\n",
        "\n",
        "  #x_train = tfidf.transform(train['data'])\n",
        "  y_train_dev= list(train['label'].values) + list(dev['label'].values)\n",
        "  #x_dev = tfidf.transform(dev['data'])\n",
        "  #y_dev= dev['label']\n",
        "\n",
        "  text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "  text_classifier.fit(x_train_dev, y_train_dev)\n",
        "\n",
        "  print(\"RandomForest Result Char on Train\")\n",
        "  print(classification_report(y_train_dev,text_classifier.predict(x_train_dev) ))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(\"svm Result char on Test\")\n",
        "  #print(classification_report(svm.predict(x_dev),y_dev))\n",
        "  return text_classifier.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW87u__cEHZF"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7KvcZ3dEJZM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from bert import bert_tokenization\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDVaniJwEMPJ"
      },
      "source": [
        "def get_model(model_url, max_seq_length):\n",
        "  inputs = dict(\n",
        "    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    )\n",
        "\n",
        "  muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "  outputs = muril_layer(inputs)\n",
        "\n",
        "  assert 'sequence_output' in outputs\n",
        "  assert 'pooled_output' in outputs\n",
        "  assert 'encoder_outputs' in outputs\n",
        "  assert 'default' in outputs\n",
        "  return tf.keras.Model(inputs=inputs,outputs=outputs[\"pooled_output\"]), muril_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czxeFmqoEOHI"
      },
      "source": [
        "max_seq_length = 100\n",
        "muril_model, muril_layer = get_model(\n",
        "    model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCxO1N8EPek"
      },
      "source": [
        "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZey7vapEQri"
      },
      "source": [
        "def create_input(input_strings, tokenizer, max_seq_length):\n",
        "  input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
        "  for input_string in input_strings:\n",
        "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "    sequence_length = min(len(input_ids), max_seq_length)\n",
        "    \n",
        "    if len(input_ids) >= max_seq_length:\n",
        "      input_ids = input_ids[:max_seq_length]\n",
        "    else:\n",
        "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "    input_ids_all.append(input_ids)\n",
        "    input_mask_all.append(input_mask)\n",
        "    input_type_ids_all.append([0] * max_seq_length)\n",
        "  \n",
        "  return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKRCOFNESK7"
      },
      "source": [
        "def encode(input_text):\n",
        "  input_ids, input_mask, input_type_ids = create_input(input_text, \n",
        "                                                       tokenizer, \n",
        "                                                       max_seq_length)\n",
        "  inputs = dict(\n",
        "      input_word_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      input_type_ids=input_type_ids,\n",
        "  )\n",
        "  return muril_model(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HbGwz1aETpg"
      },
      "source": [
        "def MURIL_logist_regression(train_embeddings,y_train, dev_embeddings,y_dev):\n",
        "  lr = LogisticRegression(max_iter=5000)\n",
        "  ovr = OneVsRestClassifier(lr)\n",
        "  ovr.fit(train_embeddings,y_train)\n",
        "\n",
        "  print(\"Linear Regression Result MuRIL on Train\")\n",
        "  print(classification_report(y_train,ovr.predict(train_embeddings)))\n",
        "\n",
        "  print(\"Linear Regression Result MuRIL on Test\")\n",
        "  print(classification_report(y_dev, ovr.predict(dev_embeddings)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SJuahtZEZZI"
      },
      "source": [
        "def MURIL_logist_regression_test(train_dev_embeddings,y_train_dev, test_embeddings):\n",
        "  lr = LogisticRegression(max_iter=5000)\n",
        "  ovr = OneVsRestClassifier(lr)\n",
        "  ovr.fit(train_dev_embeddings,y_train_dev)\n",
        "\n",
        "  # print(\"Linear Regression Result MuRIL on Train\")\n",
        "  # print(classification_report( ovr.predict(train_embeddings) , y_train))\n",
        "\n",
        "  # print(\"Linear Regression Result MuRIL on Test\")\n",
        "  # print(classification_report( ovr.predict(dev_embeddings) , y_dev))\n",
        "  return ovr.predict(test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKJRD65rEZOs"
      },
      "source": [
        "def MURIL_SVM(train_embeddings,y_train, dev_embeddings,y_dev):\n",
        "  svm = SVC(kernel='linear',max_iter=5000) \n",
        "  svm.fit(train_embeddings,y_train)\n",
        "\n",
        "  print(\"SVM Result MuRIL on Train\")\n",
        "  print(classification_report(y_train.svm.predict(train_embeddings) ))\n",
        "\n",
        "  print(\"SVM Result MuRIL on Test\")\n",
        "  print(classification_report( y_dev.svm.predict(dev_embeddings) ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z13En_xgEdlQ"
      },
      "source": [
        "def MURIL_SVM_test(train_dev_embeddings,y_train_dev, test_embeddings):\n",
        "  svm = SVC(kernel='linear',max_iter=5000) \n",
        "  svm.fit(train_dev_embeddings,y_train_dev)\n",
        "\n",
        "  # print(\"SVM Result MuRIL on Train\")\n",
        "  # print(classification_report( svm.predict(train_embeddings) , y_train))\n",
        "\n",
        "  # print(\"SVM Result MuRIL on Test\")\n",
        "  # print(classification_report( svm.predict(dev_embeddings) , y_dev))\n",
        "  return svm.predict(test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtLomMJvEfvn"
      },
      "source": [
        "def MuRIL_embeddings(train):\n",
        "  train_embeddings =  []\n",
        "  preprocessed_text=train['data']\n",
        "  i = 0\n",
        "  for k in range(50):\n",
        "    train_embeddings.extend(encode(preprocessed_text[i:min(i+len(preprocessed_text)//49,len(preprocessed_text))]))\n",
        "    i+=len(preprocessed_text)//49\n",
        "  print(len(train_embeddings))\n",
        "  return train_embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsANUPY_GnaV"
      },
      "source": [
        "#kannada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlaOAdcDGm6U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLLzmWG9EiTB"
      },
      "source": [
        "kannada_train= pd.read_csv(\"kannada_offensive_train.csv\",names=[\"data\", \"label\"],sep=\"\\t\")\n",
        "kannada_dev= pd.read_csv(\"kannada_offensive_dev.csv\",names=[\"data\", \"label\"],sep=\"\\t\")\n",
        "kannada_test= pd.read_csv(\"kannada_offensive_test.csv\",names=[\"data\"],sep=\"\\t\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcZz_gfXEsS3"
      },
      "source": [
        "kannada_train=data_preprocess(kannada_train)\n",
        "kannada_dev=data_preprocess(kannada_dev)\n",
        "kannada_test=data_preprocess(kannada_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O5FebCGEt7v"
      },
      "source": [
        "kannada_train_embeddings =  MuRIL_embeddings(kannada_train)\n",
        "kannada_dev_embeddings =  MuRIL_embeddings(kannada_dev)\n",
        "kannada_test_embeddings =  MuRIL_embeddings(kannada_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaODt3S1E1vx"
      },
      "source": [
        "kannada_train_dev_embeddings= np.concatenate((kannada_train_embeddings, kannada_dev_embeddings), axis=0)\n",
        "len(kannada_train_dev_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmU8-aXOE3a6"
      },
      "source": [
        "frames = [kannada_train,kannada_dev]\n",
        "kannada_train_dev = pd.concat(frames)\n",
        "len(kannada_train_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anD1ZGkdFGrJ"
      },
      "source": [
        "kannada_test['label']=TFIDF_char_logistic_test(kannada_train,kannada_dev,kannada_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-ZkGISFHRH"
      },
      "source": [
        "kannada_test['label']=TFIDF_char_svm_test(kannada_train,kannada_dev,kannada_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1WVgObFIzu"
      },
      "source": [
        "kannada_test['label']=TFIDF_char_randamforest_test(kannada_train,kannada_dev,kannada_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I85d7SFnE42V"
      },
      "source": [
        "kannada_test['label']=MURIL_logist_regression_test(kannada_train_dev_embeddings,kannada_train_dev['label'],kannada_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP38DvWDE6w4"
      },
      "source": [
        "kannada_test['label']=MURIL_SVM_test(kannada_train_dev_embeddings,kannada_train_dev['label'],kannada_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoMV8H1UE_vv"
      },
      "source": [
        "kannada_test['label']=MURIL_randamforest_test(kannada_train_dev_embeddings,kannada_train_dev['label'],kannada_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7WV_ZkFFLns"
      },
      "source": [
        "kannada_test=kannada_test.drop(columns=['clean_text'])\n",
        "kannada_test.index = np.arange(1, len(kannada_test) + 1)\n",
        "kannada_test.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLTm6uHhFNle"
      },
      "source": [
        "kannada_test.to_csv('IRNLP_DAIICT_kannada.tsv', sep = '\\t',header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM8k5Gf7FQqn"
      },
      "source": [
        "#Malyalam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDag18UNFSGE"
      },
      "source": [
        "mal_train= pd.read_csv(\"mal_full_offensive_train.csv\",names=[\"data\", \"label\",\"noise\"],sep=\"\\t\")\n",
        "mal_dev= pd.read_csv(\"mal_full_offensive_dev.csv\",names=[\"data\", \"label\",\"noise\"],sep=\"\\t\")\n",
        "mal_test= pd.read_csv(\"mal_full_offensive_test.csv\",names=[\"data\"],sep=\"\\t\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaYEQb_DFW4i"
      },
      "source": [
        "mal_train_embeddings=MuRIL_embeddings(mal_train)\n",
        "mal_dev_embeddings=MuRIL_embeddings(mal_dev)\n",
        "mal_test_embeddings=MuRIL_embeddings(mal_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlinQxJqFa-y"
      },
      "source": [
        "mal_train_dev_embeddings= np.concatenate((mal_train_embeddings, mal_dev_embeddings), axis=0)\n",
        "len(mal_train_dev_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIw0N4AFFe9C"
      },
      "source": [
        "frames = [mal_train,mal_dev]\n",
        "mal_train_dev = pd.concat(frames)\n",
        "len(mal_train_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OYY0xgiF9Zm"
      },
      "source": [
        "mal_train=data_preprocess(mal_train)\n",
        "mal_dev=data_preprocess(mal_dev)\n",
        "mal_test=data_preprocess(mal_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raIZ8mf7FyFx"
      },
      "source": [
        "mal_test['label']=TFIDF_char_logistic_test(mal_train,mal_dev,mal_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKMoXfxjFyi6"
      },
      "source": [
        "mal_test['label']=TFIDF_char_svm_test(mal_train,mal_dev,mal_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJHyPJrCF3bz"
      },
      "source": [
        "mal_test['label']=TFIDF_char_randamforest_test(mal_train,mal_dev,mal_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siXMZR__FiAs"
      },
      "source": [
        "\n",
        "mal_test['label']=MURIL_logist_regression_test(mal_train_dev_embeddings,mal_train_dev['label'],mal_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnbhkH8-Fj3N"
      },
      "source": [
        "mal_test['label']=MURIL_SVM_test(mal_train_dev_embeddings,mal_train_dev['label'],mal_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZQgSuyKFnDl"
      },
      "source": [
        "mal_test['label']=MURIL_randamforest_test(mal_train_dev_embeddings,mal_train_dev['label'],mal_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcciBG4PFno9"
      },
      "source": [
        "mal_test=mal_test.drop(columns=['clean_text'])\n",
        "mal_test.index = np.arange(1, len(mal_test) + 1)\n",
        "mal_test.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_o3Mk71FqZV"
      },
      "source": [
        "mal_test.to_csv('IRNLP_DAIICT_malayalam.tsv', sep = '\\t',header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDB98Kv2GBzM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh5VTjZZF_1H"
      },
      "source": [
        "#Tamil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rem34n4NF_Nk"
      },
      "source": [
        "tamil_train= pd.read_csv(\"tamil_offensive_full_train.csv\",names=[\"data\", \"label\",\"noise\"],sep=\"\\t\")\n",
        "tamil_dev= pd.read_csv(\"tamil_offensive_full_dev.csv\",names=[\"data\", \"label\",\"noise\"],sep=\"\\t\")\n",
        "\n",
        "tamil_test= pd.read_csv(\"tamil_offensive_full_test.csv\",names=[\"data\"],sep=\"\\t\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-4nahLfGH4w"
      },
      "source": [
        "tamil_train_embeddings=MuRIL_embeddings(tamil_train)\n",
        "tamil_dev_embeddings=MuRIL_embeddings(tamil_dev)\n",
        "tamil_test_embeddings=MuRIL_embeddings(tamil_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFqdqVAKGNbT"
      },
      "source": [
        "tamil_train_dev_embeddings= np.concatenate((tamil_train_embeddings, tamil_dev_embeddings), axis=0)\n",
        "len(tamil_train_dev_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSgQXghlGRGN"
      },
      "source": [
        "frames = [tamil_train,tamil_dev]\n",
        "tamil_train_dev = pd.concat(frames)\n",
        "len(tamil_train_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm-RN6HEGXL4"
      },
      "source": [
        "tamil_test['label']=TFIDF_char_logistic_test(tamil_train,tamil_dev,tamil_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqAvaYfdGZuE"
      },
      "source": [
        "tamil_test['label']=TFIDF_char_svm_test(tamil_train,tamil_dev,tamil_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVFDvjVAGboT"
      },
      "source": [
        "tamil_test['label']=TFIDF_char_randamforest_test(tamil_train,tamil_dev,tamil_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT_t13cqGUXN"
      },
      "source": [
        "tamil_test['label']=MURIL_SVM_test(tamil_train_dev_embeddings,tamil_train_dev['label'],tamil_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqkxVj8BGWZs"
      },
      "source": [
        "tamil_test['label']=MURIL_randamforest_test(tamil_train_dev_embeddings,tamil_train_dev['label'],tamil_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsfB-JzDGSvC"
      },
      "source": [
        "tamil_test['label']=MURIL_logist_regression_test(tamil_train_dev_embeddings,tamil_train_dev['label'],tamil_test_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShGuuJ3MGe-c"
      },
      "source": [
        "tamil_test=tamil_test.drop(columns=['clean_text'])\n",
        "tamil_test.index = np.arange(1, len(tamil_test) + 1)\n",
        "tamil_test.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru2LvWObGhAz"
      },
      "source": [
        "tamil_test.to_csv('IRNLP_DAIICT_tamil.tsv', sep = '\\t',header=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}